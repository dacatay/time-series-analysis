{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "<strong>Artificial neural networks (ANNs)</strong> are computing systems inspired by the biological neural networks that constitute bilological brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming (if-then clauses).\n",
    "\n",
    "An ANN is based on a collection of connected units called <strong>Perceptrons</strong> analogous to neurons in a biological brain. Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.\n",
    "\n",
    "An ANN takes an input matrix $X$ applies a weights $W$ on the way through the network from input layer to the output layer to estimate some output $y$. This estimation process is knwon as <strong>Forward Propagation</strong>. The estimate $\\hat{y}$ is compared to the actual observation $y$ using a <strong>Cost Function</strong> $E$ which computes the sum of squared errors. \n",
    "\n",
    "If the cost function is minimized for all weight parameters the neural network learns the most effective representation of the data $\\hat{y}$ that leads the desired result $y$. That is, in every iteration the weights in the network are being adjusted in accordance to their contribution to the total error in the estimation. This process is called <strong> Back Propagation</strong>. Since $W$ is a vector\n",
    "\n",
    "## Data\n",
    "\n",
    "The training data for this **supervised regression problem** example has the form\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "3 & 5 \\\\\n",
    "5 & 1 \\\\\n",
    "10 & 2\n",
    "\\end{bmatrix}, \\hspace{2cm} y = \\begin{bmatrix}\n",
    "75 \\\\\n",
    "82 \\\\\n",
    "93\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "and further, we want to estimate $\\hat{y}$ for a data input of $\\left[8, 5\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = (hours sleeping, hours studying), y = Score on test\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.,   5.],\n",
       "       [  5.,   1.],\n",
       "       [ 10.,   2.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 75.],\n",
       "       [ 82.],\n",
       "       [ 93.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3,  1. ],\n",
       "       [ 0.5,  0.2],\n",
       "       [ 1. ,  0.4]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75],\n",
       "       [ 0.82],\n",
       "       [ 0.93]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "\n",
    "\n",
    "The very basic ingredient of any artificial neural network is the artificial neuron, called <strong>Perceptron</strong>. These neurons are connected via <strong>synapses</strong>. Synapses take the input $x_{ij}$ of a preceding neuron and multiply his value with a weight $w_{ij}$ and output, $x_{ij} w_{ij}$ the result to the next subsequent neuron in.\n",
    "\n",
    "Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Network\n",
    "\n",
    "The neural network constructed in this session has two input neurons, one for every $j$ in input vector $X$, one hidden alyer with three neurons and an output layer with one neuron, correpsonding to the dimension of the desired output. These are the hyperparameters of the model which are not updated as the network is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "The neural network is not capable of deciding the size and depth of a neural network and hence the architectual structure of the network have to be provided. What a ANN does learn are the models parameters, i. e. the weights on the synapses.\n",
    "\n",
    "Data moves through the network by a method called <strong>forward propagation</strong>, hence, such models are called <strong>feed forward networks</strong>. Each input value  $x_{ij}$ of data input data matrix $X$ needs to be multiplied by the corresponding weights $w_{ij}^{(1)}$ of matrix $W^{(1)}$ like\n",
    "\n",
    "$$\n",
    "X W^{(1)} = z^{(2)} \n",
    "$$\n",
    "\n",
    "where $Z^{(2)}$ is the calculated activity of the second layer, the sum of weighted inputs from each neuron. For the example we find\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 5 \\\\\n",
    "5 & 1 \\\\\n",
    "10 & 2\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)}\\\\\n",
    "w_{21}^{(1)} & w_{22}^{(1)} & w_{23}^{(1)} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "3w_{11}^{(1)} + 5w_{21}^{(1)} & 3w_{12}^{(1)} + 5w_{22}^{(1)} & 3w_{13}^{(1)}+ 5w_{23}^{(1)}\\\\\n",
    "5w_{11}^{(1)} + 1w_{21}^{(1)} & 5w_{12}^{(1)} + 1w_{22}^{(1)} & 5w_{13}^{(1)}+ 1w_{23}^{(1)}\\\\\n",
    "10w_{11}^{(1)} + 2w_{21}^{(1)} & 10w_{12}^{(1)} + 2w_{22}^{(1)} & 10w_{13}^{(1)}+ 2w_{23}^{(1)}\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "To each entry in the activation $z^{(2)}$ an **Activation Function** $f(z)$ is applied which transforms the weighted input data. This represents to which a neuron is \"fired up\" and further propagates the signal forward through the network. In this example the activation function is **sigmoid** and has the form\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1 + e^{(-z)}}\n",
    "$$\n",
    "\n",
    "The activation function needs to be differentiable for training the model later on. \n",
    "\n",
    "$$\n",
    "a^{(2)} = f(z^{(2)})\n",
    "$$\n",
    "\n",
    "What remains to be done is to multiply $a^{(2)}$ by the second layer weights $W^{(2)}$ \n",
    "\n",
    "$$\n",
    "z^{(3)} = a^{(2)} W^{(2)}\n",
    "$$\n",
    "\n",
    "and apply another activation function to $z^{(3)}$ to yield the estimate $\\hat{y}$\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(z^{(3)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        #Propagate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = ann.feed_forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58257799],\n",
       "       [ 0.60465201],\n",
       "       [ 0.59056006]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Using the <code>ANN</code> class to predict will yields an inaccurate result. To improve on the networks prediction one first needs to quantify the extend of how much the prediction missed the actual value using a **Cost Function**. A common cost function is the sum of squared errors\n",
    "\n",
    "$$\n",
    "E(W) = \\frac{1}{2} \\sum_{i=1}^n e_i^2 = \\frac{1}{2}\\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2\n",
    "$$\n",
    "\n",
    "where $n$ is the total number of patterns shown to the system (number of input examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller the overall cost $E$ the more accurate is the prediction. Remember that $\\hat{y}$ is ultimately a function of all model data inputs and all model synapses weights. $\\hat{y}$ can be substituted with all above functions\n",
    "\n",
    "$$\n",
    "E(W) =  \\sum_{i=1}^n \\frac{1}{2} \\left( y - f\\left(f\\left(XW^{(1)}\\right) W^{(2)}\\right) \\right)^2\n",
    "$$\n",
    "\n",
    "Since the $x_{ij}$ are fixed from the outset, the only way to improve the predicion is to adjust the weight parameters until the resulting cost is minimal. The cost minimizing parameter constellation is found with an optimization method called **Gradient Descent**\n",
    "\n",
    "The process of minimizing the cost function is referred to as **training the model**.\n",
    "\n",
    "\n",
    "The contribution of the weighted inputs to the total error are found by the partial derivatives of the loss function with respect to the weights, the <strong>gradient</strong>. This method is called <strong>gradient descent</strong>. More specifically <strong>batch gradient descent</strong> will be applied where all model weights are updated simultaneously.\n",
    "\n",
    "However, if the cost function is non-convex, the gradient descent algorithm migth get stuck in local minima instead of the desired global minimum. For that matter, the sum of squared errors is chosen as the loss function as this is a convex function.\n",
    "\n",
    "\n",
    "## Back Propagation and Perceptron Learning\n",
    "\n",
    "Given a labeld set of data $(X, y)$ an optimal setting of parameters $W$ must be found that minimizes cost function $E$., The process of propagating the estimation error $E$ back through the network is called **Back Propagation**\n",
    "\n",
    "### Second Layer derivation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(2)}} &= \\frac{\\partial}{\\partial W^{(2)}}\\frac{1}{2}\\sum  \\left(y - \\hat{y}\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum \\frac{\\partial}{\\partial W^{(2)}} \\left(y - \\hat{y}\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum  2 \\left(y - \\hat{y}\\right) \\cdot \\left(-\\frac{\\partial \\hat{y}}{\\partial W^{(2)}} \\right) \\\\\n",
    "&= - \\sum \\left(y - \\hat{y}\\right) \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= - \\sum \\left(y - \\hat{y}\\right) f'\\left( z^{(3)}\\right) \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\partial z^{(3)} / \\partial W^{(2)}$ represent the change of the third layer activity with respect to the weights of the second layer. $z^{(3)}$ Ã­s the matrix product of the second layer activity $a^{(2)}$ and second layer weights $W^{(2)}$. The activities from layer two are multiplied with their corresponding weights\n",
    "\n",
    "$$\n",
    "W^{(2)} = \\begin{bmatrix}\n",
    "w_{11}^{(2)} \\\\\n",
    "w_{21}^{(2)} \\\\\n",
    "w_{31}^{(2)} \n",
    "\\end{bmatrix}, \\hspace{2cm} a^{(2)} = \\left[a^{(2)}_1, a^{(2)}_2, a^{(2)}_3 \\right]\n",
    "$$\n",
    "\n",
    "Further,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(2)}} &=  - \\sum \\left(y - \\hat{y}\\right) f'\\left( z^{(3)}\\right) \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "-y_1 - \\hat{y}_1 \\\\\n",
    "-y_2 - \\hat{y}_2  \\\\\n",
    "-y_3 - \\hat{y}_1  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "f'( z^{(3)}_1) \\\\\n",
    "f'( z^{(3)}_2)\\\\\n",
    "f'( z^{(3)}_3)\n",
    "\\end{bmatrix} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\delta_1^{(3)} \\\\\n",
    "\\delta_2^{(3)}  \\\\\n",
    "\\delta_3^{(3)}  \n",
    "\\end{bmatrix} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\delta^{(3)} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= (a^{(2)})^T \\delta^{(3)}\\\\\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "where $\\delta^{(3)}$ is the back propagating error\n",
    "\n",
    "From the linear relation $z^{(3)} = a^{(2)} W^{(2)}$ it follows that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^{(3)}}{\\partial W^{(2)}} = a^{(2)} =\\begin{bmatrix}\n",
    "a_{11}^{(2)} &a_{12}^{(2)}&a_{13}^{(2)} \\\\\n",
    "a_{21}^{(2)} &a_{22}^{(2)}&a_{23}^{(2)}  \\\\\n",
    "a_{31}^{(2)} &a_{32}^{(2)}&a_{33}^{(2)}   \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(3)} = - \\left(y - \\hat{y}\\right) f'\\left( z^{(3)}\\right)\n",
    "$$\n",
    "\n",
    "## Third layer derivation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(1)}} &=  - \\left(y - \\hat{y}\\right) f'( z^{(3)}) \\frac{\\partial z^{(3)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\delta^{(3)} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\delta^{(3)} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} \\\\ \n",
    "&= \\delta^{(3)} \\cdot (W^{(2)})^T \\cdot \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\delta^{(3)} \\cdot (W^{(2)})^T \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}  \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\\n",
    "&= \\delta^{(3)} \\cdot (W^{(2)})^T \\cdot f'( z^{(2)})  \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\\n",
    "&= X^T \\delta^{(3)} \\cdot (W^{(2)})^T \\cdot f'( z^{(2)})\\\\\n",
    "&= X^T \\delta^{(2)}\\\\\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = X^T \\delta^{(3)} (W^{(2)})^T f'( z^{(2)})\n",
    "$$\n",
    "\n",
    "The $W$ is multiplied by the <strong>learning rate</strong> $\\nu$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "\n",
    "def computeNumericalGradient(N, X, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.costFunction(X, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.costFunction(X, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann = ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 35\n",
      "         Function evaluations: 38\n",
      "         Gradient evaluations: 38\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(ann)\n",
    "trainer.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e75365e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainer.J)\n",
    "plt.grid(1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing, Overfitting and Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
